
Reddit Scraper 



Main Files Running:
1. backend/main.py - The main FastAPI application
Contains all the endpoints (/health, /debug/detect, /reddit/banks, etc.)
Has the bank detection, issue detection, and PII redaction logic
This is the primary file being executed by uvicorn
2. backend/.env - Environment variables
Contains your Reddit API credentials:
REDDIT_CLIENT_ID
REDDIT_CLIENT_SECRET
REDDIT_USER_AGENT
3. backend/requirements.txt - Dependencies
Lists all Python packages needed (fastapi, uvicorn, asyncpraw, etc.)




# Reddit Scraper API Testing Guide

## 1. Start the server:
cd /Users/enzoweiss/Desktop/stealthbanking/stealth_app/backend
source .venv/bin/activate
uvicorn main:app --reload --port 8000

## 2. Test basic health:
curl "http://127.0.0.1:8000/health"

## 3. Check data status:
curl "http://127.0.0.1:8000/data/status" | jq .

## 4. Test ML data endpoint (with persistent storage):
# First scrape - gets new data
curl "http://127.0.0.1:8000/reddit/ml-data?subs=personalfinance&fetch_limit=10&format=json" | jq '{total_new_records, duplicates_skipped, new_data: (.new_data | length)}'

# Second scrape - should skip duplicates
curl "http://127.0.0.1:8000/reddit/ml-data?subs=personalfinance&fetch_limit=10&format=json" | jq '{total_new_records, duplicates_skipped}'

# Different subreddit - gets new data
curl "http://127.0.0.1:8000/reddit/ml-data?subs=Banking&fetch_limit=10&format=json" | jq '{total_new_records, total_all_records}'

## 5. Export stored data:
# Export as JSON
curl "http://127.0.0.1:8000/data/export?format=json" | jq '{total_records, sample_record: .data[0] | {bank_name, category, sentiment_label}}'

# Export as CSV
curl "http://127.0.0.1:8000/data/export?format=csv" | head -3

## 6. Test debug endpoint:
curl "http://127.0.0.1:8000/debug/detect?text=Chase%20app%20login%20failed" | jq .

## 7. Test features endpoint:
curl "http://127.0.0.1:8000/reddit/personalfinance/features?limit=5&include_comments=false" | jq '.items[0] | {title, banks, features, sentiment_label, category}'

## 8. Test multi-subreddit banks:
curl "http://127.0.0.1:8000/reddit/banks?subs=personalfinance%2BBanking&fetch_limit=20&per_bank_limit=2" | jq '{found_banks, total_posts: (.banks | map(length) | add)}'

## 9. Clear data (if needed):
curl -X DELETE "http://127.0.0.1:8000/data/clear"

## 10. View all endpoints:
curl "http://127.0.0.1:8000/" | jq '.endpoints[]'




## Key Features:

- **Persistent Storage**: All data saved to `scraped_data.json`
- **Deduplication**: Prevents duplicate post-bank combinations
- **ML Pipeline Ready**: Returns data in ML training format
- **Enhanced Analysis**: Sentiment analysis and topic classification
- **Export Options**: JSON and CSV formats
- **Data Management**: Status, export, and clear endpoints

## Data Format for ML Pipeline:
- index, platform, bank_name, post_text, category, sentiment_label, sentiment_score
- date, user_followers, likes, shares, replies, language, source_url